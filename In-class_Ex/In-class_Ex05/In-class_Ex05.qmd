---
title: "In-Class Exercise 5 - VAST 2024 MC1 Challenge  "
author: "Alicia Loh"
date: "11 May, 2024"
date-modified: last-modified
execute: 
  eval: true
  echo: true
  warning: false
  freeze: true
---

Learning Objectives:

-   understand tidytext framework for processing, analysing and visualising text data,

-   write function for importing multiple files into R,

-   combine multiple files into a single data frame,

-   clean and wrangle text data by using tidyverse approach,

-   visualise words with Word Cloud,

-   compute term frequency–inverse document frequency (TF-IDF) using tidytext method, and

-   visualising texts and terms relationship.

# Getting Started

## Installing and loading the required libraries

The following R packages will be used:

-   tidytext, tidyverse

-   readtext

-   quanteda

Code chunk below will be used to check if these packages have been installed and also will load them into the working R environment.

```{r}
pacman::p_load(tidytext, readtext, quanteda, tidytext)
```

## **Importing Multiple Text Files from Multiple Folders**

### **Creating a folder list**

```{r}
data_folder <- "data/articles/"
```

### **Define a function to read all files from a folder into a data frame**

```{r}
text_data <- readtext(paste0("data/articles","/*"))
```

```{r}
corpus_text <- corpus(text_data)
summary(corpus_text, 5)
```

### **Text Data Processing**

-    [`unnest_tokens()`](https://www.rdocumentation.org/packages/tidytext/versions/0.3.1/topics/unnest_tokens) of **tidytext** package is used to split the dataset into tokens

-    [`stop_words()`](https://rdrr.io/cran/tidytext/man/stop_words.html) is used to remove stop-words

```{r}
usenet_words <- text_data %>%   
  unnest_tokens(word, text) %>%   
  filter(str_detect(word, "[a-z']$"),          
         !word %in% stop_words$word)
```

The code chunk below calculates individual word frequncies to explore common words in the dataset.

```{r}
usenet_words %>%
  count(word,sort = TRUE)
```

```{r}
text_data_splitted <- text_data %>%
  separate_wider_delim("doc_id",
                       delim = "__0__",
                       names = c("X","Y"),
                       too_few = "align_end")
```

### **JSONLite**

```{r}
pacman::p_load(jsonlite, tidyverse)
```

```{r}
mc1_data <- fromJSON("data/mc1.json")
mc2_data <- fromJSON("data/mc2.json")
mc3_data <- fromJSON("data/mc3.json")
```

## To view the dataframe

```{r}
view(mc1_data[["nodes"]])
view(mc1_data[["links"]])
```
