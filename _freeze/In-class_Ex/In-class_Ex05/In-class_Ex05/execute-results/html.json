{
  "hash": "12f52581f377b2e2a9a3000f2b15dd95",
  "result": {
    "markdown": "---\ntitle: \"Hands-on Exercise 5 -  Visualising and Analysing Text Data\"\nauthor: \"Alicia Loh\"\ndate: \"9 May, 2024\"\ndate-modified: last-modified\nexecute: \n  eval: true\n  echo: true\n  warning: false\n  freeze: true\n---\n\n\nLearning Objectives:\n\n-   understand tidytext framework for processing, analysing and visualising text data,\n\n-   write function for importing multiple files into R,\n\n-   combine multiple files into a single data frame,\n\n-   clean and wrangle text data by using tidyverse approach,\n\n-   visualise words with Word Cloud,\n\n-   compute term frequency–inverse document frequency (TF-IDF) using tidytext method, and\n\n-   visualising texts and terms relationship.\n\n# Getting Started\n\n## Installing and loading the required libraries\n\nThe following R packages will be used:\n\n-   tidytext, tidyverse (mainly readr, purrr, stringr, ggplot2)\n\n-   widyr,\n\n-   wordcloud and ggwordcloud,\n\n-   textplot (required igraph, tidygraph and ggraph, )\n\n-   DT,\n\n-   lubridate and hms.\n\nCode chunk below will be used to check if these packages have been installed and also will load them into the working R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(tidytext, widyr, wordcloud, DT, ggwordcloud, textplot, lubridate, hms,tidyverse, tidygraph, ggraph, igraph)\n```\n:::\n\n\n## **Importing Multiple Text Files from Multiple Folders**\n\n### **Creating a folder list**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnews20 <- \"data/20news/\"\n```\n:::\n\n\n### **Define a function to read all files from a folder into a data frame**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_folder <- function(infolder) {\n  tibble(file = dir(infolder, \n                    full.names = TRUE)) %>%\n    mutate(text = map(file, \n                      read_lines)) %>%\n    transmute(id = basename(file), \n              text) %>%\n    unnest(text)\n}\n```\n:::\n\n\n## **Importing Multiple Text Files from Multiple Folders**\n\n### **Reading in all the messages from the 20news folder**\n\n-   [`read_lines()`](https://readr.tidyverse.org/reference/read_lines.html) of [**readr**](https://readr.tidyverse.org/) package is used to read up to n_max lines from a file.\n\n-   [`map()`](https://purrr.tidyverse.org/reference/map.html) of [**purrr**](https://purrr.tidyverse.org/) package is used to transform their input by applying a function to each element of a list and returning an object of the same length as the input.\n\n-   [`unnest()`](https://tidyr.tidyverse.org/reference/nest.html) of **dplyr** package is used to flatten a list-column of data frames back out into regular columns.\n\n-   [`mutate()`](https://dplyr.tidyverse.org/reference/mutate.html) of **dplyr** is used to add new variables and preserves existing ones;\n\n-   [`transmute()`](https://dplyr.tidyverse.org/reference/mutate.html) of **dplyr** is used to add new variables and drops existing ones.\n\n-   [`read_rds()`](https://readr.tidyverse.org/reference/read_rds.html) is used to save the extracted and combined data frame as rds file for future use.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_text <- tibble(folder = \n                     dir(news20, \n                         full.names = TRUE)) %>%\n  mutate(folder_out = map(folder, \n                          read_folder)) %>%\n  unnest(cols = c(folder_out)) %>%\n  transmute(newsgroup = basename(folder), \n            id, text)\nwrite_rds(raw_text, \"data/rds/news20.rds\")\n```\n:::\n\n\n## **Initial EDA**\n\nFigure below shows the frequency of messages by newsgroup.\n\n::: panel-tabset\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_text <- read_rds(\"data/rds/news20.rds\")\nraw_text %>%\n  group_by(newsgroup) %>%\n  summarize(messages = n_distinct(id)) %>%\n  ggplot(aes(messages, newsgroup)) +\n  geom_col(fill = \"lightblue\") +\n  labs(y = NULL)\n```\n:::\n\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_text <- read_rds(\"data/rds/news20.rds\")\nraw_text %>%\n  group_by(newsgroup) %>%\n  summarize(messages = n_distinct(id)) %>%\n  ggplot(aes(messages, newsgroup)) +\n  geom_col(fill = \"lightblue\") +\n  labs(y = NULL)\n```\n:::\n\n:::\n\n## **Introducing tidytext**\n\n-   Using tidy data principles in processing, analysing and visualising text data.\n\n-   Much of the infrastructure needed for text mining with tidy data frames already exists in packages like ‘dplyr’, ‘broom’, ‘tidyr’, and ‘ggplot2’.\n\n### **Removing header and automated email signitures**\n\nEach message contains certain structural elements and additional text that are undesirable for inclusion in the analysis. For example:\n\n-   Header containing fields such as “from:” or “in_reply_to:”\n\n-   Automated email signatures, which occur after a line like “–”.\n\nThe code chunk below uses:\n\n-   [`cumsum()`](https://rdrr.io/r/base/cumsum.html) of base R to return a vector whose elements are the cumulative sums of the elements of the argument.\n\n-   [`str_detect()`](https://stringr.tidyverse.org/reference/str_detect.html) from **stringr** to detect the presence or absence of a pattern in a string.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncleaned_text <- raw_text %>%\n  group_by(newsgroup, id) %>%\n  filter(cumsum(text == \"\") > 0,\n         cumsum(str_detect(\n           text, \"^--\")) == 0) %>%\n  ungroup()\n```\n:::\n\n\n### **Removing lines with nested text representing quotes from other users**\n\nRegular expressions are used to remove with nested text representing quotes from other users.\n\n-   [`str_detect()`](https://stringr.tidyverse.org/reference/str_detect.html) from **stringr** is used to detect the presence or absence of a pattern in a string.\n\n-   [`filter()`](https://dplyr.tidyverse.org/reference/filter.html) of **dplyr** package is used to subset a data frame, retaining all rows that satisfy the specified conditions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncleaned_text <- cleaned_text %>%\n  filter(str_detect(text, \"^[^>]+[A-Za-z\\\\d]\")\n         | text == \"\",\n         !str_detect(text, \n                     \"writes(:|\\\\.\\\\.\\\\.)$\"),\n         !str_detect(text, \n                     \"^In article <\")\n  )\n```\n:::\n\n\n### **Text Data Processing**\n\n-    [`unnest_tokens()`](https://www.rdocumentation.org/packages/tidytext/versions/0.3.1/topics/unnest_tokens) of **tidytext** package is used to split the dataset into tokens\n\n-    [`stop_words()`](https://rdrr.io/cran/tidytext/man/stop_words.html) is used to remove stop-words\n\n\n::: {.cell}\n\n```{.r .cell-code}\nusenet_words <- cleaned_text %>%\n  unnest_tokens(word, text) %>%\n  filter(str_detect(word, \"[a-z']$\"),\n         !word %in% stop_words$word)\n```\n:::\n\n\nHeaders, signatures and formatting have been removed. The code chunk below calculates individual word frequncies to explore common words in the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nusenet_words %>%\n  count(word, sort = TRUE)\n```\n:::\n\n\nWord frequencies within newsgroup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwords_by_newsgroup <- usenet_words %>%\n  count(newsgroup, word, sort = TRUE) %>%\n  ungroup()\n```\n:::\n\n\n### **Visualising Words in newsgroups**\n\n-   `wordcloud()` of **wordcloud** package is used to plot a static wordcloud\n\n::: panel-tabset\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwordcloud(words_by_newsgroup$word,\n          words_by_newsgroup$n,\n          max.words = 300)\n```\n:::\n\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwordcloud(words_by_newsgroup$word,\n          words_by_newsgroup$n,\n          max.words = 300)\n```\n:::\n\n:::\n\nA DT table can be used to complement the visual discovery.\n\n::: panel-tabset\n## Table\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a data frame with word frequency data\nword_freq_table <- data.frame(Word = words_by_newsgroup$word,\n                              Frequency = words_by_newsgroup$n)\n\n# Render the DataTable\ndatatable(word_freq_table, \n          options = list(pageLength = 10))\n```\n:::\n\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a data frame with word frequency data\nword_freq_table <- data.frame(Word = words_by_newsgroup$word,\n                              Frequency = words_by_newsgroup$n)\n\n# Render the DataTable\ndatatable(word_freq_table, \n          options = list(pageLength = 10))\n```\n:::\n\n:::\n\n### **Visualising Words in newsgroups**\n\n [**ggwordcloud**](https://lepennec.github.io/ggwordcloud/) package is used to plot the wordcloud below\n\n::: panel-tabset\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\nwords_by_newsgroup %>%\n  filter(n > 0) %>%\nggplot(aes(label = word,\n           size = n)) +\n  geom_text_wordcloud() +\n  theme_minimal() +\n  facet_wrap(~newsgroup)\n```\n:::\n\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\nwords_by_newsgroup %>%\n  filter(n > 0) %>%\nggplot(aes(label = word,\n           size = n)) +\n  geom_text_wordcloud() +\n  theme_minimal() +\n  facet_wrap(~newsgroup)\n```\n:::\n\n:::\n\n## **Basic Concept of TF-IDF**\n\n[tf–idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection of [corpus](https://en.wikipedia.org/wiki/Text_corpus).\n\n$idf(term) = ln \\frac{n_{documents}}{n_{documents containing term}}$\n\n### **Computing tf-idf within newsgroups**\n\n*`bind_tf_idf()`* of ***tidytext*** is used to compute and bind the term frequency, inverse document frequency and ti-idf of a tidy text dataset to the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntf_idf <- words_by_newsgroup %>%\n  bind_tf_idf(word, newsgroup, n) %>%\n  arrange(desc(tf_idf))\n```\n:::\n\n\n### **Visualising tf-idf as interactive table**\n\nInteractive table created by using [`datatable()`](https://rstudio.github.io/DT/functions.html) to create a html table that allows pagination of rows and columns.\n\nThe code chunk below also uses:\n\n-   `filter()` argument is used to turn control the filter UI.\n\n-   `formatRound()` is used to customise the values format. The argument *digits* define the number of decimal places.\n\n-   `formatStyle()` is used to customise the output table. In this example, the arguments *target* and *lineHeight* are used to reduce the line height by 25%.\n\n::: panel-tabset\n## Table\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDT::datatable(tf_idf, filter = 'top') %>% \n  formatRound(columns = c('tf', 'idf', \n                          'tf_idf'), \n              digits = 3) %>%\n  formatStyle(0, \n              target = 'row', \n              lineHeight='25%')\n```\n:::\n\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDT::datatable(tf_idf, filter = 'top') %>% \n  formatRound(columns = c('tf', 'idf', \n                          'tf_idf'), \n              digits = 3) %>%\n  formatStyle(0, \n              target = 'row', \n              lineHeight='25%')\n```\n:::\n\n:::\n\n### **Visualising tf-idf within newsgroups**\n\nFacet bar charts technique is used to visualise the tf-idf values of science related newsgroup.\n\n::: panel-tabset\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntf_idf %>%\n  filter(str_detect(newsgroup, \"^sci\\\\.\")) %>%\n  group_by(newsgroup) %>%\n  slice_max(tf_idf, \n            n = 12) %>%\n  ungroup() %>%\n  mutate(word = reorder(word, \n                        tf_idf)) %>%\n  ggplot(aes(tf_idf, \n             word, \n             fill = newsgroup)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ newsgroup, \n             scales = \"free\") +\n  labs(x = \"tf-idf\", \n       y = NULL)\n```\n:::\n\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntf_idf %>%\n  filter(str_detect(newsgroup, \"^sci\\\\.\")) %>%\n  group_by(newsgroup) %>%\n  slice_max(tf_idf, \n            n = 12) %>%\n  ungroup() %>%\n  mutate(word = reorder(word, \n                        tf_idf)) %>%\n  ggplot(aes(tf_idf, \n             word, \n             fill = newsgroup)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ newsgroup, \n             scales = \"free\") +\n  labs(x = \"tf-idf\", \n       y = NULL)\n```\n:::\n\n:::\n\n### **Counting and correlating pairs of words with the widyr package**\n\n-   To count the number of times that two words appear within the same document, or to see how correlated they are.\n\n-   Most operations for finding pairwise counts or correlations need to turn the data into a wide matrix first.\n\n-   [**widyr**](https://cran.r-project.org/web/packages/widyr/) package first ‘casts’ a tidy dataset into a wide matrix, performs an operation such as a correlation on it, then re-tidies the result.\n\nIn this code chunk below, `pairwise_cor()` of **widyr** package is used to compute the correlation between newsgroup based on the common words found.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewsgroup_cors <- words_by_newsgroup %>%\n  pairwise_cor(newsgroup, \n               word, \n               n, \n               sort = TRUE)\n```\n:::\n\n\n### **Visualising correlation as a network**\n\nRelationship between newgroups is visualised as a network graph\n\n::: panel-tabset\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2017)\n\nnewsgroup_cors %>%\n  filter(correlation > .025) %>%\n  graph_from_data_frame() %>%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(alpha = correlation, \n                     width = correlation)) +\n  geom_node_point(size = 6, \n                  color = \"lightblue\") +\n  geom_node_text(aes(label = name),\n                 color = \"red\",\n                 repel = TRUE) +\n  theme_void()\n```\n:::\n\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2017)\n\nnewsgroup_cors %>%\n  filter(correlation > .025) %>%\n  graph_from_data_frame() %>%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(alpha = correlation, \n                     width = correlation)) +\n  geom_node_point(size = 6, \n                  color = \"lightblue\") +\n  geom_node_text(aes(label = name),\n                 color = \"red\",\n                 repel = TRUE) +\n  theme_void()\n```\n:::\n\n:::\n\n### **Bigram**\n\nCreated by using `unnest_tokens()` of tidytext.\n\n::: panel-tabset\n## Bigram\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbigrams <- cleaned_text %>%\n  unnest_tokens(bigram, \n                text, \n                token = \"ngrams\", \n                n = 2)\n\nbigrams\n```\n:::\n\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbigrams <- cleaned_text %>%\n  unnest_tokens(bigram, \n                text, \n                token = \"ngrams\", \n                n = 2)\n\nbigrams\n```\n:::\n\n:::\n\n### **Counting bigrams**\n\nCount and sort the bigram data frame ascendingly\n\n::: panel-tabset\n## Bigram Count\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbigrams_count <- bigrams %>%\n  filter(bigram != 'NA') %>%\n  count(bigram, sort = TRUE)\n\nbigrams_count\n```\n:::\n\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbigrams_count <- bigrams %>%\n  filter(bigram != 'NA') %>%\n  count(bigram, sort = TRUE)\n\nbigrams_count\n```\n:::\n\n:::\n\n### **Cleaning bigram**\n\nSeperate the bigram into two words\n\n::: panel-tabset\n## Bigram\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbigrams_separated <- bigrams %>%\n  filter(bigram != 'NA') %>%\n  separate(bigram, c(\"word1\", \"word2\"), \n           sep = \" \")\n\nbigrams_filtered <- bigrams_separated %>%\n  filter(!word1 %in% stop_words$word) %>%\n  filter(!word2 %in% stop_words$word)\n\nbigrams_filtered\n```\n:::\n\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbigrams_separated <- bigrams %>%\n  filter(bigram != 'NA') %>%\n  separate(bigram, c(\"word1\", \"word2\"), \n           sep = \" \")\n\nbigrams_filtered <- bigrams_separated %>%\n  filter(!word1 %in% stop_words$word) %>%\n  filter(!word2 %in% stop_words$word)\n```\n:::\n\n:::\n\n### **Counting the bigram again**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbigram_counts <- bigrams_filtered %>% \n  count(word1, word2, sort = TRUE)\n```\n:::\n\n\n### **Create a network graph from bigram data frame**\n\nA network graph is created by using `graph_from_data_frame()` of **igraph** package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbigram_graph <- bigram_counts %>%\n  filter(n > 3) %>%\n  graph_from_data_frame()\nbigram_graph\n```\n:::\n\n\n### **Visualizing a network of bigrams with ggraph**\n\n**ggraph** package is used to plot the bigram\n\n::: panel-tabset\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\nggraph(bigram_graph, layout = \"fr\") +\n  geom_edge_link() +\n  geom_node_point() +\n  geom_node_text(aes(label = name), \n                 vjust = 1, \n                 hjust = 1)\n```\n:::\n\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\nggraph(bigram_graph, layout = \"fr\") +\n  geom_edge_link() +\n  geom_node_point() +\n  geom_node_text(aes(label = name), \n                 vjust = 1, \n                 hjust = 1)\n```\n:::\n\n:::\n\n### **Revised version**\n\n::: panel-tabset\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\na <- grid::arrow(type = \"closed\", \n                 length = unit(.15,\n                               \"inches\"))\n\nggraph(bigram_graph, \n       layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n), \n                 show.legend = FALSE,\n                 arrow = a, \n                 end_cap = circle(.07,\n                                  'inches')) +\n  geom_node_point(color = \"lightblue\", \n                  size = 5) +\n  geom_node_text(aes(label = name), \n                 vjust = 1, \n                 hjust = 1) +\n  theme_void()\n```\n:::\n\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\na <- grid::arrow(type = \"closed\", \n                 length = unit(.15,\n                               \"inches\"))\n\nggraph(bigram_graph, \n       layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n), \n                 show.legend = FALSE,\n                 arrow = a, \n                 end_cap = circle(.07,\n                                  'inches')) +\n  geom_node_point(color = \"lightblue\", \n                  size = 5) +\n  geom_node_text(aes(label = name), \n                 vjust = 1, \n                 hjust = 1) +\n  theme_void()\n```\n:::\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}